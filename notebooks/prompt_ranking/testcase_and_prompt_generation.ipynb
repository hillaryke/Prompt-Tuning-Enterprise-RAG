{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path path/to/your/document.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _create_rag_chain\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Load documents and create vectorstore\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mload_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath/to/your/document.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m db \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(docs, OpenAIEmbeddings())\n\u001b[1;32m     89\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt 2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt 3\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Replace with your actual prompts\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 55\u001b[0m, in \u001b[0;36mload_docs\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_docs\u001b[39m(file_path):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents from a PDF file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     57\u001b[0m     text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/code/10Academy-training/week7/Prompt-Tuning-Enterprise-RAG/.venv/lib/python3.9/site-packages/langchain_community/document_loaders/pdf.py:182\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[0;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf package not found, please install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m     )\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(password\u001b[38;5;241m=\u001b[39mpassword, extract_images\u001b[38;5;241m=\u001b[39mextract_images)\n",
      "File \u001b[0;32m~/code/10Academy-training/week7/Prompt-Tuning-Enterprise-RAG/.venv/lib/python3.9/site-packages/langchain_community/document_loaders/pdf.py:116\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[0;31mValueError\u001b[0m: File path path/to/your/document.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Install required libraries:\n",
    "# !pip install pypdf2 unstructured chromadb sentence-transformers langchain langchain_openai tiktoken pypdf faiss-cpu openai\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "def generate_test_cases(description, model_name=\"gpt-3.5-turbo\", temperature=1.5, amount=10):\n",
    "    \"\"\"Generates test cases for a given task description using an LLMChain.\"\"\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are a helpful assistant who generates test cases for LLM applications.\n",
    "\n",
    "    Please create {amount} test case scenarios for the following task description, each with a clear expected output:\n",
    "\n",
    "    Task: {description}\n",
    "\n",
    "    Format each test case like this:\n",
    "    Scenario: [Scenario description]\n",
    "    Expected output: [Expected output of the model]\n",
    "    \"\"\")\n",
    "\n",
    "    chain = LLMChain(llm=ChatOpenAI(model_name=model_name, temperature=temperature), prompt=prompt_template)\n",
    "\n",
    "    response = chain.run({\"description\": description, \"amount\": amount})\n",
    "\n",
    "    test_cases = []\n",
    "    for case_str in response.split(\"Scenario:\"):\n",
    "        if not case_str.strip():  # Skip empty lines\n",
    "            continue\n",
    "        scenario, expected_output = case_str.split(\"Expected output:\", 1)\n",
    "        test_cases.append(\n",
    "            {\"prompt\": scenario.strip(), \"expected_output\": expected_output.strip()}\n",
    "        )\n",
    "\n",
    "    return test_cases\n",
    "\n",
    "\n",
    "def evaluate_answer(answer, test_case):\n",
    "    \"\"\"Evaluates an answer against the expected output of a test case.\"\"\"\n",
    "    # Placeholder for evaluation logic (you'll need to implement this)\n",
    "    # Example: You could use a similarity metric like ROUGE or BLEU to compare the texts\n",
    "    return {\"score\": len(answer)}  # Just a dummy score for demonstration\n",
    "\n",
    "\n",
    "def load_docs(file_path):\n",
    "    \"\"\"Load documents from a PDF file.\"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "def create_rag_chain(prompt_template, db):\n",
    "    retriever = db.as_retriever(search_type = \"similarity\")\n",
    "\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt_template}\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "# Example usage\n",
    "description = \"Explain the concept of few-shot learning.\"\n",
    "test_cases = generate_test_cases(description)\n",
    "\n",
    "# Function to create a RAG chain with a given prompt\n",
    "def create_rag_chain_factory(prompt_template, db):\n",
    "    \"\"\"Creates a RAG chain factory function with the specified prompt and vectorstore.\"\"\"\n",
    "    def _create_rag_chain(prompt):\n",
    "        return create_rag_chain(prompt_template, db)\n",
    "    return _create_rag_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Prompt #1: Prompt: Describe how few-shot learning enables machines to quickly adapt to new tasks with minimal training examples, showcasing its efficiency in learning diverse concepts with limited data.\n",
      "Candidate Prompt #2: Retrieve documents that provide an in-depth explanation of the concept of few-shot learning, focusing on how machines can quickly adapt to new tasks with minimal training examples. Be sure to include examples that showcase the practical applications and benefits of this approach.\n",
      "Candidate Prompt #3: Prompt: Describe how few-shot learning allows machines to quickly adapt to new tasks with minimal training examples.\n",
      "Candidate Prompt #4: Prompt: Describe how few-shot learning enables machines to quickly adapt to new tasks with minimal training examples.\n",
      "Candidate Prompt #5: Prompt: Retrieve documents that provide a comprehensive explanation of few-shot learning, including its core principles and applications in various domains.\n",
      "Candidate Prompt #6: Prompt: Provide an overview of a machine learning technique that allows models to quickly learn new tasks with minimal training examples.\n",
      "Candidate Prompt #7: Prompt: Describe how few-shot learning enables machines to quickly adapt and learn new tasks with minimal training examples, showcasing its efficiency in leveraging prior knowledge for rapid task comprehension.\n",
      "Candidate Prompt #8: Prompt: Retrieve documents that provide an in-depth explanation of few-shot learning and how it enables machines to quickly adapt to new tasks with minimal training data.\n",
      "Candidate Prompt #9: Explore the concept of few-shot learning with a focus on its applications and advantages.\n",
      "Candidate Prompt #10: Prompt: Describe how few-shot learning allows machines to quickly adapt to new tasks with minimal training examples, highlighting its benefits and applications in various domains.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def generate_prompt_candidates(test_cases, description, amount=10):\n",
    "    \"\"\"Generates prompt candidates using a chat-based language model.\"\"\"\n",
    "    prompt_generation_model = \"gpt-3.5-turbo\"  # Or another suitable model\n",
    "    system_messages = [\n",
    "        \"Generate a diverse and creative prompt that can retrieve relevant documents.\",\n",
    "        \"Create an engaging and precise prompt for document retrieval.\",\n",
    "        \"Craft a prompt that effectively guides the retrieval of relevant documents.\",\n",
    "    ]  # Add more system messages for variation\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    for i in range(amount):\n",
    "        system_message = system_messages[i % len(system_messages)]\n",
    "\n",
    "        # Format test cases for the prompt\n",
    "        formatted_test_cases = \"\\n\".join(\n",
    "            [\n",
    "                f\"Test case #{j+1}:\\nScenario: {test_case['prompt']}\\nExpected output: {test_case['expected_output']}\"\n",
    "                for j, test_case in enumerate(test_cases)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant skilled at creating prompts for retrieving relevant documents. \n",
    "\n",
    "        Consider these test cases:\n",
    "\n",
    "        {formatted_test_cases}\n",
    "\n",
    "        Here's the goal for the final prompt:\n",
    "        {description}\n",
    "\n",
    "        Please create a prompt that would effectively guide the retrieval of relevant documents for similar scenarios.\n",
    "\n",
    "        Constraints:\n",
    "        * Do NOT include any specific details from the test cases in your prompt.\n",
    "        * The prompt should be clear, concise, and easy to understand.\n",
    "        * IF YOU USE EXAMPLES, ALWAYS USE ONES THAT ARE VERY DIFFERENT FROM THE TEST CASES.\n",
    "        \"\"\"\n",
    "\n",
    "        model = ChatOpenAI(model=prompt_generation_model, temperature=1 if i > 0 else 0)\n",
    "\n",
    "        try:\n",
    "            response = model.invoke(system_message + prompt)\n",
    "            candidate = response.content.strip() \n",
    "\n",
    "            if candidate and candidate not in candidates:  # Avoid duplicates\n",
    "                candidates.append(candidate)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating prompt candidate: {e}\")\n",
    "            # You might want to handle this error more gracefully in a production setting\n",
    "\n",
    "    return candidates\n",
    "\n",
    "# Example Usage:\n",
    "description = \"Explain the concept of few-shot learning.\"\n",
    "test_cases = generate_test_cases(description) # Replace this with your actual test cases\n",
    "candidates = generate_prompt_candidates(test_cases, description)\n",
    "\n",
    "for i, candidate in enumerate(candidates):\n",
    "    print(f\"Candidate Prompt #{i+1}: {candidate}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prompt: Describe how few-shot learning enables machines to quickly adapt to new tasks with minimal training examples, showcasing its efficiency in learning diverse concepts with limited data.',\n",
       " 'Retrieve documents that provide an in-depth explanation of the concept of few-shot learning, focusing on how machines can quickly adapt to new tasks with minimal training examples. Be sure to include examples that showcase the practical applications and benefits of this approach.',\n",
       " 'Prompt: Describe how few-shot learning allows machines to quickly adapt to new tasks with minimal training examples.',\n",
       " 'Prompt: Describe how few-shot learning enables machines to quickly adapt to new tasks with minimal training examples.',\n",
       " 'Prompt: Retrieve documents that provide a comprehensive explanation of few-shot learning, including its core principles and applications in various domains.',\n",
       " 'Prompt: Provide an overview of a machine learning technique that allows models to quickly learn new tasks with minimal training examples.',\n",
       " 'Prompt: Describe how few-shot learning enables machines to quickly adapt and learn new tasks with minimal training examples, showcasing its efficiency in leveraging prior knowledge for rapid task comprehension.',\n",
       " 'Prompt: Retrieve documents that provide an in-depth explanation of few-shot learning and how it enables machines to quickly adapt to new tasks with minimal training data.',\n",
       " 'Explore the concept of few-shot learning with a focus on its applications and advantages.',\n",
       " 'Prompt: Describe how few-shot learning allows machines to quickly adapt to new tasks with minimal training examples, highlighting its benefits and applications in various domains.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a single prompt candidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import context_precision, faithfulness, answer_relevancy, context_recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompts(test_cases, candidates, rag_chain_factory):\n",
    "    # ... (Your other functions like retrieve_context and create_rag_chain_factory remain the same) ...\n",
    "    results = []\n",
    "    for test_case in test_cases:\n",
    "        question = test_case[\"prompt\"]\n",
    "        expected_output = test_case[\"expected_output\"]\n",
    "\n",
    "        context = retrieve_context(question)\n",
    "        retrieved_docs = [Document(page_content=doc) for doc in context]\n",
    "\n",
    "        for candidate in candidates:\n",
    "            # Extract the prompt text by removing the \"Prompt: \" prefix\n",
    "            prompt = candidate[len(\"Prompt: \"):].strip() \n",
    "\n",
    "            rag_chain = rag_chain_factory(prompt)\n",
    "\n",
    "            chain = (\n",
    "                {\"context\": retrieved_docs, \"question\": RunnablePassthrough()}\n",
    "                | rag_chain.prompt\n",
    "                | rag_chain.llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                answer = chain.invoke(test_case[\"prompt\"])\n",
    "                # Update candidate's rating based on answer quality (runBattle logic)\n",
    "                # ...\n",
    "\n",
    "                data = {\n",
    "                    \"question\": [question],\n",
    "                    \"answer\": [answer],\n",
    "                    \"contexts\": [\n",
    "                        retrieved_docs,\n",
    "                    ],\n",
    "                }\n",
    "\n",
    "                scores = evaluate(\n",
    "                    data,\n",
    "                    metrics=[\n",
    "                        context_precision,\n",
    "                        faithfulness,\n",
    "                        answer_relevancy,\n",
    "                        context_recall,\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"test_case\": test_case,\n",
    "                        \"candidate\": candidate,\n",
    "                        \"answer\": answer,\n",
    "                        \"scores\": scores,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating prompt '{candidate}': {e}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\"prompt\": \"What is few shot learning?\", \"expected_output\": \"Few-shot learning is a machine learning approach where a model can learn to perform a new task with only a few examples, while traditional machine learning typically requires large amounts of labeled data.\"},\n",
    "    {\"prompt\": \"Give an example of how few-shot learning could be applied in a real-world application.\", \"expected_output\": \"Few-shot learning could be used to develop a medical diagnosis system that can quickly learn to identify new diseases with limited data.\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"/home/hilla/code/10Academy-training/week7/Prompt-Tuning-Enterprise-RAG/notebooks/prompt_ranking/Download Llama Terms and policy.pdf\") # Adjust to your document loader\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "db = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "\n",
    "def retrieve_context(query):\n",
    "    \"\"\"Retrieves relevant context for a given query from your knowledge base or documents.\"\"\"\n",
    "    # Replace with your actual retrieval logic using your RAG chain.\n",
    "    similar_docs = db.similarity_search(query)\n",
    "\n",
    "    return [doc.page_content for doc in similar_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RetrievalQA' object has no attribute 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m rag_chain_factory(prompt)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# ... (evaluation steps below) ...\u001b[39;00m\n\u001b[1;32m     35\u001b[0m chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     36\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: retrieved_docs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()}\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;241m|\u001b[39m rag_chain\u001b[38;5;241m.\u001b[39mllm\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Generate answer from RAG chain\u001b[39;00m\n\u001b[1;32m     43\u001b[0m answer \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke(test_case[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RetrievalQA' object has no attribute 'prompt'"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "def create_rag_chain_factory(prompt_template, db):\n",
    "    def _create_rag_chain(prompt):\n",
    "        return create_rag_chain(prompt_template, db)\n",
    "\n",
    "    return _create_rag_chain\n",
    "\n",
    "prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]  # Replace with your actual prompts\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Use the following context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "rag_chain_factory = create_rag_chain_factory(prompt_template, db)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for test_case in test_cases[:2]:  # Test on the first two test cases for demonstration\n",
    "    question = test_case[\"prompt\"]\n",
    "    expected_output = test_case[\"expected_output\"]\n",
    "\n",
    "    # Use the function here to get context\n",
    "    context = retrieve_context(question)\n",
    "    retrieved_docs = [Document(page_content=doc) for doc in context]\n",
    "\n",
    "    for candidate in candidates:\n",
    "        # Extract the prompt text by removing the \"Prompt: \" prefix\n",
    "        prompt = candidate[len(\"Prompt: \"):].strip()\n",
    "\n",
    "        rag_chain = rag_chain_factory(prompt)\n",
    "        # ... (evaluation steps below) ...\n",
    "\n",
    "        chain = (\n",
    "            {\"context\": retrieved_docs, \"question\": RunnablePassthrough()}\n",
    "            | rag_chain.prompt\n",
    "            | rag_chain.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # Generate answer from RAG chain\n",
    "        answer = chain.invoke(test_case[\"prompt\"])\n",
    "        print(answer)\n",
    "\n",
    "        data = {\n",
    "            \"question\": [question],\n",
    "            \"answer\": [answer],\n",
    "            \"contexts\": [\n",
    "                retrieved_docs,\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        scores = evaluate(\n",
    "            data,\n",
    "            metrics=[\n",
    "                context_precision,\n",
    "                faithfulness,\n",
    "                answer_relevancy,\n",
    "                context_recall,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"test_case\": test_case,\n",
    "                \"candidate\": candidate,\n",
    "                \"answer\": answer,\n",
    "                \"scores\": scores,\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_distribution(candidates, sample_amount):\n",
    "    \"\"\"Initializes the distribution for Monte Carlo simulation.\"\"\"\n",
    "    distribution = {str(candidate[\"id\"]): 0 for candidate in candidates}\n",
    "    return distribution\n",
    "\n",
    "def run_monte_carlo_simulation(candidates, distribution, sample_amount):\n",
    "    \"\"\"Performs Monte Carlo simulation to estimate prompt rankings.\"\"\"\n",
    "    for _ in range(sample_amount):\n",
    "        samples = {\n",
    "            str(candidate[\"id\"]): randomNormal(\n",
    "                mean=candidate[\"rating\"], dev=candidate[\"sd\"]\n",
    "            )\n",
    "            for candidate in candidates\n",
    "        }\n",
    "        winner = max(samples, key=samples.get)  # Get the prompt with the highest sample\n",
    "        distribution[winner] += 1\n",
    "    return distribution\n",
    "\n",
    "def randomly_select_from_distribution(distribution, excluded=\"\"):\n",
    "    \"\"\"Randomly selects a prompt from the distribution, excluding the 'excluded' prompt.\"\"\"\n",
    "    filtered_dist = {k: v for k, v in distribution.items() if k != excluded}\n",
    "    total = sum(filtered_dist.values())\n",
    "    rand = random.uniform(0, total)\n",
    "    for prompt_id, count in filtered_dist.items():\n",
    "        rand -= count\n",
    "        if rand <= 0:\n",
    "            return prompt_id\n",
    "\n",
    "def create_battle(test_cases, prompt_a, prompt_b):\n",
    "    \"\"\"Creates a new battle object for the selected prompts.\"\"\"\n",
    "    return {\n",
    "        \"a\": prompt_a,\n",
    "        \"b\": prompt_b,\n",
    "        \"rounds\": [{\"testCase\": tc, \"result\": None, \"generation\": {\"a\": '', \"b\": ''}} for tc in test_cases],\n",
    "        \"winner\": None,\n",
    "    }\n",
    "\n",
    "#... (rest of code)\n",
    "\n",
    "async def run_battle():\n",
    "    sample_amount = 1000 # Replace with value from useSettings() hook\n",
    "    learning_rate = 0.9 # Replace with value from useSettings() hook\n",
    "    distribution = initialize_distribution(candidates.value, sample_amount)\n",
    "    distribution = await run_monte_carlo_simulation(candidates.value, distribution, sample_amount)\n",
    "\n",
    "    prompt_a = randomly_select_from_distribution(distribution)\n",
    "    prompt_b = randomly_select_from_distribution(distribution, prompt_a)\n",
    "\n",
    "    new_battle = create_battle(testCases.value, prompt_a, prompt_b)\n",
    "    \n",
    "    #... (rest of run_battle code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 69\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# ... (assumes test_cases, description, and a single candidate prompt are already defined) ...\u001b[39;00m\n\u001b[1;32m     67\u001b[0m candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour prompt goes here\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# The prompt you want to evaluate\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_single_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_rag_chain_factory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m evaluation_results:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Case: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_case\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m, in \u001b[0;36mevaluate_single_prompt\u001b[0;34m(prompt, test_cases, rag_chain_factory)\u001b[0m\n\u001b[1;32m     14\u001b[0m question \u001b[38;5;241m=\u001b[39m test_case[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m expected_output \u001b[38;5;241m=\u001b[39m test_case[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_context\u001b[49m(question)\n\u001b[1;32m     18\u001b[0m retrieved_docs \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mdoc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m context]\n\u001b[1;32m     20\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m rag_chain_factory(prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retrieve_context' is not defined"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import context_precision, faithfulness, answer_relevancy, context_recall\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def evaluate_single_prompt(prompt, test_cases, rag_chain_factory):\n",
    "    \"\"\"Evaluates the quality of answers generated by a single prompt candidate using RAGAs metrics.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        question = test_case[\"prompt\"]\n",
    "        expected_output = test_case[\"expected_output\"]\n",
    "\n",
    "        context = retrieve_context(question)\n",
    "        retrieved_docs = [Document(page_content=doc) for doc in context]\n",
    "\n",
    "        rag_chain = rag_chain_factory(prompt)\n",
    "\n",
    "        chain = (\n",
    "            {\"context\": retrieved_docs, \"question\": RunnablePassthrough()}\n",
    "            | rag_chain.prompt\n",
    "            | rag_chain.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            answer = chain.invoke(test_case[\"prompt\"])\n",
    "\n",
    "            data = {\n",
    "                \"question\": [question],\n",
    "                \"answer\": [answer],\n",
    "                \"contexts\": [\n",
    "                    retrieved_docs,\n",
    "                ],\n",
    "            }\n",
    "\n",
    "            scores = evaluate(\n",
    "                data,\n",
    "                metrics=[\n",
    "                    context_precision,\n",
    "                    faithfulness,\n",
    "                    answer_relevancy,\n",
    "                    context_recall,\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"test_case\": test_case,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"answer\": answer,\n",
    "                    \"scores\": scores,\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating prompt '{prompt}': {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# ... (assumes test_cases, description, and a single candidate prompt are already defined) ...\n",
    "\n",
    "candidate = \"Your prompt goes here\"  # The prompt you want to evaluate\n",
    "\n",
    "evaluation_results = evaluate_single_prompt(candidate, test_cases, create_rag_chain_factory)\n",
    "\n",
    "for result in evaluation_results:\n",
    "    print(f\"Test Case: {result['test_case']['prompt']}\")\n",
    "    print(f\"Prompt: {result['prompt']}\")\n",
    "    print(f\"Generated Answer: {result['answer']}\")\n",
    "    print(f\"Scores: {result['scores']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3920130440.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    def evaluate_prompts(test_cases, candidates, rag_chain_factory):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import context_precision, faithfulness, answer_relevancy, context_recall\n",
    "import pLimit\n",
    "import randomNormal\n",
    "import resguard\n",
    "from utils import timeout # Make sure you have this module if you use it in you original code\n",
    "from utils.types import Candidate #Make sure you have this module if you use it in you original code\n",
    "from utils.synced_state import useSyncedState #Make sure you have this module if you use it in you original code\n",
    "import random\n",
    "\n",
    "# ... (Your RAG chain creation function and `retrieve_context` function from previous responses) ...\n",
    "\n",
    "def generate_prompt_candidates(test_cases, description, amount=10):\n",
    "    # ... (same as the provided generate_prompt_candidates function) ...\n",
    "\n",
    "def evaluate_prompts(test_cases, candidates, rag_chain_factory):\n",
    "    \"\"\"Evaluates a list of prompt candidates using RAGAs metrics and the provided RAG chain factory.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        question = test_case[\"prompt\"]\n",
    "        expected_output = test_case[\"expected_output\"]\n",
    "\n",
    "        context = retrieve_context(question)\n",
    "        retrieved_docs = [Document(page_content=doc) for doc in context]\n",
    "\n",
    "        for candidate in candidates:\n",
    "            rag_chain = rag_chain_factory(candidate)\n",
    "\n",
    "            chain = (\n",
    "                {\"context\": retrieved_docs, \"question\": RunnablePassthrough()}\n",
    "                | rag_chain.prompt\n",
    "                | rag_chain.llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                answer = chain.invoke(test_case[\"prompt\"])\n",
    "                \n",
    "                data = {\n",
    "                    \"question\": [question],\n",
    "                    \"answer\": [answer],\n",
    "                    \"contexts\": [\n",
    "                        retrieved_docs,\n",
    "                    ],\n",
    "                }\n",
    "\n",
    "                scores = evaluate(\n",
    "                    data,\n",
    "                    metrics=[\n",
    "                        context_precision,\n",
    "                        faithfulness,\n",
    "                        answer_relevancy,\n",
    "                        context_recall,\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"test_case\": test_case,\n",
    "                        \"candidate\": candidate,\n",
    "                        \"answer\": answer,\n",
    "                        \"scores\": scores,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating prompt '{candidate}': {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# ... (assumes test_cases, description, and candidates are already defined) ...\n",
    "# Create RAG chain factory\n",
    "def create_rag_chain_factory(prompt_template, db):\n",
    "    def _create_rag_chain(prompt):\n",
    "        return create_rag_chain(prompt_template, db)\n",
    "\n",
    "    return _create_rag_chain\n",
    "# Load documents and create vectorstore\n",
    "docs = load_docs(\"path/to/your/document.pdf\")\n",
    "db = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "\n",
    "prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]  # Replace with your actual prompts\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Use the following context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "rag_chain_factory = create_rag_chain_factory(prompt_template, db)\n",
    "evaluation_results = evaluate_prompts(test_cases, candidates, rag_chain_factory)\n",
    "for result in evaluation_results:\n",
    "    print(f\"Test Case: {result['test_case']['prompt']}\")\n",
    "    print(f\"Candidate: {result['candidate']}\")\n",
    "    print(f\"Generated Answer: {result['answer']}\")\n",
    "    print(f\"Scores: {result['scores']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RagasEvaluatorChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse the following context to answer the question at the end. If you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer, just say that you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know, don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt try to make up an answer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m rag_chain_factory \u001b[38;5;241m=\u001b[39m create_rag_chain_factory(prompt_template, db)\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrag_chain_factory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Replace the placeholder evaluation with your actual logic (e.g., ROUGE, BLEU, etc.)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Example: Print the scores for each prompt and test case\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt, scores \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36mevaluate_prompts\u001b[0;34m(test_cases, candidates, rag_chain_factory)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_prompts\u001b[39m(test_cases, candidates, rag_chain_factory):\n\u001b[0;32m----> 2\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mRagasEvaluatorChain\u001b[49m(\n\u001b[1;32m      3\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[faithfulness, answer_relevancy, context_precision, context_recall]\n\u001b[1;32m      4\u001b[0m     )\n\u001b[1;32m      5\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m test_case \u001b[38;5;129;01min\u001b[39;00m test_cases:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RagasEvaluatorChain' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load documents and create vectorstore\n",
    "docs = load_docs(\"./Download Llama Terms and policy.pdf\")\n",
    "db = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "\n",
    "prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]  # Replace with your actual prompts\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Use the following context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "rag_chain_factory = create_rag_chain_factory(prompt_template, db)\n",
    "results = evaluate_prompts(prompts, test_cases, rag_chain_factory)\n",
    "\n",
    "# Replace the placeholder evaluation with your actual logic (e.g., ROUGE, BLEU, etc.)\n",
    "# Example: Print the scores for each prompt and test case\n",
    "for prompt, scores in results.items():\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    with get_openai_callback() as cb:\n",
    "        for i, score in enumerate(scores):\n",
    "            print(f\"  Test Case #{i + 1}: {score}\")\n",
    "            print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "            print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "            print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "            print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI  # Assuming you're using OpenAI's models\n",
    "\n",
    "\n",
    "class TestCase:\n",
    "    def __init__(self, prompt, expected_output=None, id=None):\n",
    "        self.prompt = prompt\n",
    "        self.expected_output = expected_output\n",
    "        self.id = id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test cases using chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case #1:\n",
      "  Scenario: A student is researching on machine learning and comes across the term few-shot learning for the first time.\n",
      "  Expected Output: The system should explain that few-shot learning is a type of machine learning where a model can learn from very limited training data, usually only a few examples per class, in order to make precise predictions for new unseen data. It should also mention techniques such as meta-learning and transfer learning that are commonly used in few-shot learning approaches.\n",
      "\n",
      "Test Case #2:\n",
      "  Scenario: A software engineer working on developing an image recognition algorithm wants to understand how few-shot learning works.\n",
      "  Expected Output: The system should provide a detailed explanation that in few-shot learning, the model is trained with a very small dataset compared to traditional machine learning models. It utilizes transfer learning, meta-learning, or other techniques to learn from few examples in order to quickly adapt to new information and classification tasks.\n",
      "\n",
      "Test Case #3:\n",
      "  Scenario: A business executive is interested in exploring how few-shot learning can improve their company's customer service chatbot capabilities.\n",
      "  Expected Output: The system should explain how few-shot learning can enhance the chatbot's ability to quickly learn new information or conversation styles with just a few examples, making it easier to tailor the responses to specific customer queries or trends. It should outline potential benefits such as improved customer satisfaction and reduced response time.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "class TestCase:\n",
    "    def __init__(self, scenario, expected_output=None, id=None):\n",
    "        self.scenario = scenario\n",
    "        self.expected_output = expected_output\n",
    "        self.id = id\n",
    "        \n",
    "def generate_test_cases(task_description, amount=3):\n",
    "    \"\"\"Generates test cases using a language model based on the task description.\"\"\"\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    You are a helpful AI assistant. Please generate {amount} diverse test cases for the following task:\n",
    "\n",
    "    Task: {description}\n",
    "\n",
    "    Each test case should include:\n",
    "    * Scenario: A clear description of the situation or input to be tested.\n",
    "    * Expected output: The ideal or expected output from the system.\n",
    "\n",
    "    Format each test case like this:\n",
    "    Scenario: [Scenario description]\n",
    "    Expected output: [Expected output]\n",
    "    \"\"\"\n",
    "\n",
    "    model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1.5)\n",
    "\n",
    "    messages = [\n",
    "        HumanMessage(\n",
    "            content=prompt_template.format(amount=amount, description=task_description)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    response = model(messages)\n",
    "    generated_text = response.content\n",
    "\n",
    "    test_cases = []\n",
    "    for case_str in generated_text.split(\"Scenario:\"):\n",
    "        case_str = case_str.strip() # Ensure that there are no spaces \n",
    "        if not case_str:\n",
    "            continue\n",
    "        try:\n",
    "            scenario, expected_output = case_str.split(\"Expected output:\", 1)\n",
    "            test_cases.append(\n",
    "                TestCase(scenario.strip(), expected_output.strip())\n",
    "            )\n",
    "        except ValueError:\n",
    "            print(f\"Skipping malformed test case: {case_str}\")  # Handle cases where \"Expected output:\" is missing\n",
    "\n",
    "    return test_cases\n",
    "\n",
    "\n",
    "# Example usage\n",
    "task_description = \"Explain the concept of few-shot learning.\"\n",
    "generated_test_cases = generate_test_cases(task_description)\n",
    "for i, test_case in enumerate(generated_test_cases):\n",
    "    print(f\"\\nTest Case #{i + 1}:\")\n",
    "    print(f\"  Scenario: {test_case.scenario}\")\n",
    "    print(f\"  Expected Output: {test_case.expected_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of prompt candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = generated_test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_generation_model = \"gpt-3.5-turbo\"  # Or another suitable model\n",
    "temperature = 1.0  # Adjust for creativity (higher values = more creative)\n",
    "num_prompt_candidates = 3  # Number of prompts to generate\n",
    "\n",
    "# This emulates the system messages from the JavaScript code\n",
    "system_messages = [\n",
    "    \"You are an AI assistant skilled at creating prompts for retrieving relevant documents.\",\n",
    "    \"Generate a concise and effective prompt to guide document retrieval.\",\n",
    "    \"Craft a prompt that focuses on extracting specific information from documents.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an AI assistant skilled at creating prompts for retrieving relevant documents. \n",
    "\n",
    "Consider these test cases:\n",
    "\n",
    "{test_cases}\n",
    "\n",
    "Here's the goal for the final prompt:\n",
    "{description}\n",
    "\n",
    "Please create a prompt that would effectively guide the retrieval of relevant documents for similar scenarios.\n",
    "\n",
    "Constraints:\n",
    "* Do NOT include any specific details from the test cases in your prompt.\n",
    "* The prompt should be clear, concise, and easy to understand.\n",
    "* IF YOU USE EXAMPLES, ALWAYS USE ONES THAT ARE VERY DIFFERENT FROM THE TEST CASES.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.TestCase at 0x7c86b06ea880>,\n",
       " <__main__.TestCase at 0x7c86b06ea910>,\n",
       " <__main__.TestCase at 0x7c86b06c7ca0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An experienced data scientist is interested in practical applications of few-shot learning in the industry.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_test_cases[2].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario: The user is new to the concept of few-shot learning and wants a basic explanation.\n",
      "Expected output: A concise definition of few-shot learning as a machine learning technique that involves training a model using a minimal amount of labelled examples, allowing it to make predictions with only a few examples.\n",
      "\n",
      "Test Case 2:\n",
      "\n",
      "Scenario: A student in a machine learning course wants to understand the difference between few-shot learning and traditional supervised learning.\n",
      "Expected output: An explanation that few-shot learning differs from traditional supervised learning in that it focuses on training models to learn from a small number of samples, as opposed to large amounts of labelled data typically required in traditional supervised learning.\n",
      "\n",
      "Test Case 3:\n",
      "\n",
      "Scenario: An experienced data scientist is interested in practical applications of few-shot learning in the industry.\n",
      "Expected output: Examples of real-world applications of few-shot learning such as personalized medical diagnoses, few-shot image recognition tasks, and quick adaptation of models to new tasks with minimal training data.\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(generated_test_cases)):\n",
    "#     print(f\"\\nScenario: {generated_test_cases[i].prompt}\\nExpected output: {generated_test_cases[i].expected_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A clear explanation that indicates few-shot learning as the process of training a model on a small dataset, often just a few examples, to make predictions on new, unseen data.\\n\\nTest Case 2:'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_test_cases[0].expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cases_list = []\n",
    "\n",
    "# for i in range(len(generated_test_cases)):\n",
    "#     test_cases_list.append(f\"Test case #{i+1}:\\nScenario: {generated_test_cases[i].prompt}\\nExpected output: {generated_test_cases[i].expected_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_test_cases_str = '\\n'.join(test_cases_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain.chat_models import ChatOpenAI  # Import the correct class for chat models\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# TestCase class (unchanged)\n",
    "# class TestCase(BaseModel):\n",
    "#     prompt: str\n",
    "#     expected_output: str\n",
    "\n",
    "# System messages and model name\n",
    "system_messages = [\n",
    "    \"You are an AI assistant skilled at creating prompts for getting information from relevant documents.\",\n",
    "    \"Generate a concise and effective prompt to guide information retrieval from documents.\",\n",
    "    \"Craft a prompt that focuses on extracting specific information from documents.\"\n",
    "]\n",
    "prompt_generation_model_name = \"gpt-3.5-turbo\"  # Use the correct model name\n",
    "\n",
    "\n",
    "def generate_prompt_candidates(testcases: List[TestCase], description: str) -> List[str]:\n",
    "    \"\"\"Generates prompt candidates using a chat language model.\"\"\"\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    for i in range(5):\n",
    "        system_message = system_messages[i % len(system_messages)]\n",
    "\n",
    "        formatted_test_cases_str = \"\\n\".join(\n",
    "            [\n",
    "                f\"Test case #{j+1}:\\nScenario: {testcase.prompt}\\nExpected output: {testcase.expected_output}\"\n",
    "                for j, testcase in enumerate(testcases)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        prompt_content = f\"\"\"\n",
    "        You are an AI assistant skilled at creating prompts for retrieving relevant documents. \n",
    "        \n",
    "        Here are some test case scenarios and their expected outputs:\n",
    "        {formatted_test_cases_str}\n",
    "\n",
    "        Here is what the user wants the final prompt to accomplish:\n",
    "        {description}\n",
    "        \n",
    "        Respond with your prompt, and nothing else. Be creative.\n",
    "        NEVER CHEAT BY INCLUDING SPECIFICS ABOUT THE TEST CASES IN YOUR PROMPT. \n",
    "        ANY PROMPTS WITH THOSE SPECIFIC EXAMPLES WILL BE DISQUALIFIED.\n",
    "        IF YOU USE EXAMPLES, ALWAYS USE ONES THAT ARE VERY DIFFERENT FROM THE TEST CASES.\n",
    "        \"\"\"\n",
    "\n",
    "        temperature = 0.7 if i > 0 else 0 # First prompt deterministic, rest creative\n",
    "\n",
    "        # Use ChatOpenAI for chat models\n",
    "        model = ChatOpenAI(model_name=prompt_generation_model_name, temperature=temperature)  \n",
    "        messages = [\n",
    "            SystemMessage(content=system_message),\n",
    "            HumanMessage(content=prompt_content)\n",
    "        ]\n",
    "\n",
    "        response = model(messages)\n",
    "        candidates.append(response.content.strip())  # Extract the content directly\n",
    "\n",
    "    return candidates\n",
    "\n",
    "# ... (rest of your code) ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Prompt #1: Describe the unique approach in machine learning that involves learning from a limited number of examples.\n",
      "Candidate Prompt #2: Describe the principles and techniques involved in a unique approach to machine learning that aims to minimize the amount of training data required.\n",
      "Candidate Prompt #3: Please provide detailed explanations, definitions, and examples to help clarify the concept of few-shot learning.\n",
      "Candidate Prompt #4: Describe the unique approach of a learning method that involves acquiring knowledge from only a limited amount of data.\n",
      "Candidate Prompt #5: Describe the principles and applications of few-shot learning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage:\n",
    "description = \"Explain the concept of few-shot learning.\"\n",
    "\n",
    "candidates = generate_prompt_candidates(generated_test_cases, description)\n",
    "\n",
    "for i, candidate in enumerate(candidates):\n",
    "    print(f\"Candidate Prompt #{i+1}: {candidate}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Describe the unique approach in machine learning that involves learning from a limited number of examples.',\n",
       " 'Describe the principles and techniques involved in a unique approach to machine learning that aims to minimize the amount of training data required.',\n",
       " 'Please provide detailed explanations, definitions, and examples to help clarify the concept of few-shot learning.',\n",
       " 'Describe the unique approach of a learning method that involves acquiring knowledge from only a limited amount of data.',\n",
       " 'Describe the principles and applications of few-shot learning.']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get generation of answer for the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# def getGeneration(prompt: str, testCase: TestCase):\n",
    "#     \"\"\"Generates an answer using the provided prompt and test case.\"\"\"\n",
    "    \n",
    "#     # Retrieve relevant context\n",
    "#     context = retrieve_context(testCase.prompt)\n",
    "#     # Create RAG chain for current prompt\n",
    "#     rag_chain = create_rag_chain_factory(prompt, db)\n",
    "#     chain = (\n",
    "#                 {\"context\": context, \"question\": RunnablePassthrough()}\n",
    "#                 | rag_chain.prompt\n",
    "#                 | rag_chain.llm\n",
    "#                 | StrOutputParser()\n",
    "#             )\n",
    "    \n",
    "#     # Generate answer from RAG chain\n",
    "#     answer = chain.invoke(testCase.prompt)\n",
    "#     return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getGeneration(prompt: str, test_case: TestCase, model: ChatOpenAI):\n",
    "    \"\"\"Generates a response using the provided prompt and test case.\"\"\"\n",
    "\n",
    "    # Construct the user message with the prompt and test case question\n",
    "    user_message = f\"{prompt}\\n{test_case.prompt}\"\n",
    "\n",
    "    # Prepare messages for the chat model\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an assistant to help in generating helpful answers.\"),  # Or your preferred system message\n",
    "        HumanMessage(content=user_message),\n",
    "    ]\n",
    "    \n",
    "    # Get the answer from RAG chain\n",
    "    # Create RAG chain for current prompt\n",
    "    rag_chain = create_rag_chain_factory(prompt, db)\n",
    "    chain = (\n",
    "                {\"context\": retrieved_docs, \"question\": RunnablePassthrough()}\n",
    "                | rag_chain.prompt\n",
    "                | rag_chain.llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "    \n",
    "    # Generate answer from RAG chain\n",
    "    answer = chain.invoke(test_case.prompt)\n",
    "    # Get the response from the model\n",
    "    response = await model.agenerate(messages)\n",
    "\n",
    "    # Extract and return the generated content\n",
    "    return response.generations[0][0].text or \"\"  # Handle empty responses gracefully\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the scores for the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import asyncio\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "def get_score(test_case: TestCase, prompt_a: str, prompt_b: str, model: ChatOpenAI, embedding_model: OpenAIEmbeddings):\n",
    "    \"\"\"Calculates the score for a prompt comparison using human judgment or embedding similarity.\"\"\"\n",
    "    answer_a = asyncio.run(getGeneration(prompt_a, test_case, model))\n",
    "    answer_b = asyncio.run(getGeneration(prompt_b, test_case, model))\n",
    "\n",
    "    if not test_case.expected_output.strip():  # No expected output, rely on human judgment\n",
    "        print(f\"Test Case: {test_case.prompt}\")\n",
    "        print(f\"Option A: {answer_a}\")\n",
    "        print(f\"Option B: {answer_b}\")\n",
    "\n",
    "        winner = input(\"Which answer is better? (A/B/draw): \").upper()\n",
    "        while winner not in ['A', 'B', 'DRAW']:\n",
    "            winner = input(\"Invalid input. Please enter A, B, or draw: \").upper()\n",
    "\n",
    "        return 1 if winner == 'A' else 0 if winner == 'B' else 0.5\n",
    "    else:  # Use embeddings to calculate similarity\n",
    "        embedding_a = embedding_model.embed_query(answer_a)\n",
    "        embedding_b = embedding_model.embed_query(answer_b)\n",
    "        embedding_expected = embedding_model.embed_query(test_case.expected_output)\n",
    "\n",
    "        score_a = cosine_similarity(embedding_a, embedding_expected)\n",
    "        score_b = cosine_similarity(embedding_b, embedding_expected)\n",
    "\n",
    "        # Handle ties and near-ties\n",
    "        if abs(score_a - score_b) < 0.1:  # Adjust threshold as needed\n",
    "            return 0.5  # Draw\n",
    "\n",
    "        return 1 if score_a > score_b else 0  # Return 1 if A wins, 0 if B wins\n",
    "    \n",
    "# Cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split into chunks and save them to the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=200)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a rag chain factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.chat import ChatPromptTemplate\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# def create_rag_chain_factory(prompt_template, vectorstore):\n",
    "#     retriever = vectorstore.as_retriever() # This assumes that you've already loaded your vector database into the db variable\n",
    "    \n",
    "#     # Here you would typically use a RetrievalQA chain, but for simplicity, \n",
    "#     # we are just returning the prompt\n",
    "    \n",
    "#     rag_chain = RetrievalQA.from_chain_type(\n",
    "#         llm=ChatOpenAI(temperature=0),\n",
    "#         chain_type=\"stuff\",\n",
    "#         retriever=retriever,\n",
    "#         return_source_documents=True,\n",
    "#         chain_type_kwargs={\"prompt\": prompt_template}\n",
    "#     )\n",
    "    \n",
    "#     return rag_chain\n",
    "\n",
    "# example usage of the create_rag_chain_factory\n",
    "# prompt_template = ChatPromptTemplate.from_template(\"Prompt: {prompt}\")\n",
    "# rag_chain_factory = create_rag_chain_factory(prompt_template, vectorstore=vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def retrieve_context(query, retriever):\n",
    "    \"\"\"Retrieves relevant context for a given query from your knowledge base or documents.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return docs\n",
    "\n",
    "def getGeneration(prompt_candidate: str, testCase: TestCase, model: ChatOpenAI, retriever):\n",
    "    \"\"\"Generates a response using the provided prompt and test case.\"\"\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        Use the following context to answer the question.\n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        Context: {context}\n",
    "        Question: {prompt_candidate}\n",
    "        Scenario: {scenario}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    # print(testCase.scenario)\n",
    "    context = retrieve_context(testCase.scenario, retriever) # get the context documents \n",
    "\n",
    "    # Create RAG chain for current prompt\n",
    "    # rag_chain = create_rag_chain_factory(prompt, vectorstore)\n",
    "    chain = prompt_template | model | StrOutputParser()\n",
    "    # Generate answer from RAG chain\n",
    "    answer = chain.invoke({\"context\": context, \"prompt_candidate\": prompt_candidate, \"scenario\": testCase.scenario})\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A student is researching on machine learning and comes across the term few-shot learning for the first time.'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[0]\n",
    "generated_test_cases[0].scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test getGeneration function\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "answer = getGeneration(candidates[0], generated_test_cases[0], llm, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Few-shot learning is a unique approach in machine learning where the model is trained on a limited number of examples or demonstrations, each consisting of both input and desired output on the target task. By providing the model with good examples, it can better understand human intention and criteria for the desired answers. This approach often leads to better performance compared to zero-shot learning, where the model has no prior examples to learn from. However, few-shot learning may require more token consumption and could hit the context length limit when dealing with long input and output text.'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_embedding(text, model):\n",
    "    \"\"\"Gets the embedding for a given text.\"\"\"\n",
    "    return model.embed_query(text)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculates the cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def get_score(test_case, prompt_a, prompt_b, model, embedding_model):\n",
    "    \"\"\"Calculates the score for a prompt comparison using either human judgment or embedding similarity.\"\"\"\n",
    "    answer_a = asyncio.run(getGeneration(prompt_a, test_case, model))\n",
    "    answer_b = asyncio.run(getGeneration(prompt_b, test_case, model))\n",
    "\n",
    "    if not test_case.expected_output.strip():  # No expected output, rely on human judgment\n",
    "        print(f\"Test Case: {test_case.prompt}\")\n",
    "        print(f\"Option A: {answer_a}\")\n",
    "        print(f\"Option B: {answer_b}\")\n",
    "\n",
    "        winner = input(\"Which answer is better? (A/B/draw): \").upper()\n",
    "        while winner not in ['A', 'B', 'DRAW']:\n",
    "            winner = input(\"Invalid input. Please enter A, B, or draw: \").upper()\n",
    "\n",
    "        return 1 if winner == 'A' else 0 if winner == 'B' else 0.5\n",
    "\n",
    "    else:  # Use embeddings to calculate similarity\n",
    "        embedding_a = get_embedding(answer_a, embedding_model)\n",
    "        embedding_b = get_embedding(answer_b, embedding_model)\n",
    "        embedding_expected = get_embedding(test_case.expected_output, embedding_model)\n",
    "\n",
    "        score_a = cosine_similarity(embedding_a, embedding_expected)\n",
    "        score_b = cosine_similarity(embedding_b, embedding_expected)\n",
    "\n",
    "        # Handle ties and near-ties\n",
    "        if abs(score_a - score_b) < 0.1:  # Adjust threshold as needed\n",
    "            return 0.5  # Draw\n",
    "\n",
    "        return 1 if score_a > score_b else 0  # Return 1 if A wins, 0 if B wins\n",
    "\n",
    "# Initialize Langchain models\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "# Sample Usage to Test get_score()\n",
    "test_case = test_cases[0]  # Choose the first test case\n",
    "prompt_a = candidates[0]  # Choose the first prompt candidate\n",
    "prompt_b = candidates[1]  # Choose the second prompt candidate\n",
    "\n",
    "#Get scores\n",
    "score = get_score(test_case, prompt_a, prompt_b, model, embedding_model)\n",
    "\n",
    "print(\"Scores:\", score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
