{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.env_loader import load_api_key\n",
    "api_key = load_api_key(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "# api_key = load_api_key(\"OPENAI_API_KEY_NEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chatbot\n",
    "chat = ChatOpenAI(model=model_name, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_dotenv\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chatbot\n",
    "chat = ChatOpenAI(model=model_name, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour, tu te sens bien?', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 20, 'total_tokens': 27}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-449978b9-13cc-4c52-94af-37f47ee8a205-0', usage_metadata={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chatbot\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this text to French: Hello, do you feel good?\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I haven't said anything yet. How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None}, id='run-75df461d-274c-4154-9bf8-ec2287ba48c0-0', usage_metadata={'input_tokens': 13, 'output_tokens': 13, 'total_tokens': 26})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no state/memory => no follow-up question and hence not conversation\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"What did you just say?\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated \"Hello, do you feel good?\" to French as \"Bonjour, tu te sens bien?\"', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 41, 'total_tokens': 62}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None}, id='run-1465343f-80f2-460c-99f2-b5538faf2e8b-0', usage_metadata={'input_tokens': 41, 'output_tokens': 21, 'total_tokens': 62})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#provide the history\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this text to French: Hello, do you feel good?\"\n",
    "        ),\n",
    "        AIMessage(\n",
    "            content=\"Bonjour, tu te sens bien?\",\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=\"What did you just say?\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Exampel with PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1. Justin Bieber was born on March 1, 1994.\\n2. The Super Bowl is typically held in January or February of each year.\\n3. Therefore, the Super Bowl that took place during the year of Justin Bieber's birth would have been Super Bowl XXVIII, held on January 30, 1994.\\n4. Super Bowl XXVIII was won by the Dallas Cowboys, who defeated the Buffalo Bills with a final score of 30-13.\\n5. Therefore, the Dallas Cowboys are the NFL team that won the Super Bowl in the year Justin Bieber was born.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using chat openai with chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9WgYfBV3Le2LP8uoQWGv1MS3ly1tr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Bonjour, comment ça va ?', role='assistant', function_call=None, tool_calls=None))], created=1717577117, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_319be4768e', usage=CompletionUsage(completion_tokens=6, prompt_tokens=30, total_tokens=36))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chat completion\n",
    "# - The chat completion endpoint is used to simulate a conversation between a user and a model.\n",
    "# - The model will respond to the user's messages based on the context of the conversation.\n",
    "# - The model will generate a response to the user's messages based on the context of the conversation.\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Translate this English text to French: Hello, how are you?\"},\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Bonjour, comment ça va ?', role='assistant', function_call=None, tool_calls=None))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour, comment ça va ?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
