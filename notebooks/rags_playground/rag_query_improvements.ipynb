{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag From Scratch: Query Transformations\n",
    "\n",
    "Query transformations are a set of approaches focused on re-writing and / or modifying questions for retrieval.\n",
    "\n",
    "## Enviornment\n",
    "\n",
    "`(1) Packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) LangSmith`\n",
    "\n",
    "https://docs.smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "# os.environ['LANGCHAIN_API_KEY'] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['OPENAI_API_KEY'] = <your-api-key>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Multi Query\n",
    "\n",
    "Flow:\n",
    "\n",
    "Docs:\n",
    "\n",
    "* https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\n",
    "\n",
    "### Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate {num_of_prompts_to_generate} \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['num_of_prompts_to_generate', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['num_of_prompts_to_generate', 'question'], template='You are an AI language model assistant. Your task is to generate {num_of_prompts_to_generate} \\ndifferent versions of the given user question to retrieve relevant documents from a vector \\ndatabase. By generating multiple perspectives on the user question, your goal is to help\\nthe user overcome some of the limitations of the distance-based similarity search. \\nProvide these alternative questions separated by newlines. Original question: {question}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x729e4d9c4b80>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x729e4da31580>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "| StrOutputParser()\n",
       "| RunnableLambda(...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hilla/code/10Academy-training/week7/Prompt-Tuning-Enterprise-RAG/.venv/lib/python3.9/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is few shot?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question, \"num_of_prompts_to_generate\": 5})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "quiz_template = \"\"\"\n",
    "You are an assistant for bank_name bank products and answer customer questions.\n",
    "Use fragments of the received context to answer the question.\n",
    "If you don't know the answer, say that you don't know, don't make up an answer.\n",
    "Use a maximum of three sentences and be concise.\\n\n",
    "Question: {question} \\n\n",
    "Context: {context} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(quiz_template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "      \"context\": retriever, \n",
    "      \"question\": RunnablePassthrough()\n",
    "     } \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain.invoke(\"What is few shot?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Few-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Few-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question, \"num_of_prompts_to_generate\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate answers for each prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_questions = [\"What is few shot?\", \"Explain the concept of few shot learning?\", \"What is the largest ocean on Earth?\"]\n",
    "\n",
    "answers = [{f\"{question}\": rag_chain.invoke(question)} for question in test_questions ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the scores of each prompt to evaluate it's performance - Use sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hilla/code/10Academy-training/week7/Prompt-Tuning-Enterprise-RAG/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is few shot?\n",
      "Answer: Few-shot learning involves providing a model with a few high-quality demonstrations to improve its understanding of human intention and criteria for desired answers. This method often leads to better performance than zero-shot learning but may require more token consumption and could hit context length limits with long input and output text. For more information, you can refer to the source: https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/.\n",
      "Score: 0.6792625188827515\n",
      "\n",
      "\n",
      "Question: Explain the concept of few shot learning?\n",
      "Answer: Few-shot learning involves providing the model with a few high-quality demonstrations to better understand human intention and criteria for desired answers. This approach often leads to better performance than zero-shot learning but may consume more tokens and hit context length limits with long input and output text. Instruction prompting is used to explain the task intent to the model and improve alignment with human intention through fine-tuning with high-quality tuples of task instruction, input, and ground truth output.\n",
      "Score: 0.7300750017166138\n",
      "\n",
      "\n",
      "Question: What is the largest ocean on Earth?\n",
      "Answer: I don't know the answer to that question.\n",
      "Score: 0.08854614943265915\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "for item in data:\n",
    "    for question, answer in item.items():\n",
    "        # Generate embeddings\n",
    "        question_embedding = model.encode([question])\n",
    "        answer_embedding = model.encode([answer])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        score = cosine_similarity(question_embedding, answer_embedding)\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"Score: {score[0][0]}\")\n",
    "        print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test cases or test scenarios for comparing prompts (the scenarios are just situatations and not actual questions\n",
    "# while the expected output of each scenario is the general expectation of what a prompt should look like in that situation)\n",
    "\n",
    "test_scenarios = [\n",
    "    {\n",
    "      \"scenario\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELO ratings: {'What is few shot?': 1000, 'Explain the concept of few shot learning?': 1000, 'What is the largest ocean on Earth?': 1000}\n"
     ]
    }
   ],
   "source": [
    "def update_elo_ratings(rating1, rating2, score):\n",
    "  \"\"\"\n",
    "  Update ELO ratings based on the score of a game.\n",
    "\n",
    "  Parameters:\n",
    "  rating1 (float): The ELO rating of player 1.\n",
    "  rating2 (float): The ELO rating of player 2.\n",
    "  score (float): The score of the game. 1 if player 1 wins, 0 if player 2 wins, 0.5 for a draw.\n",
    "\n",
    "  Returns:\n",
    "  float: The updated ELO rating of player 1.\n",
    "  float: The updated ELO rating of player 2.\n",
    "  \"\"\"\n",
    "  # Calculate expected scores\n",
    "  expected_score1 = 1 / (1 + 10 ** ((rating2 - rating1) / 400))\n",
    "  expected_score2 = 1 / (1 + 10 ** ((rating1 - rating2) / 400))\n",
    "\n",
    "  # Update ratings\n",
    "  k = 32\n",
    "  rating1 = rating1 + k * (score - expected_score1)\n",
    "  rating2 = rating2 + k * ((1 - score) - expected_score2)\n",
    "\n",
    "  return rating1, rating2\n",
    "# Number of rounds\n",
    "num_rounds = 10\n",
    "\n",
    "# Initialize ELO ratings\n",
    "elo_ratings = {question: 1000 for question in test_questions}\n",
    "\n",
    "# Calculate scores and update ELO ratings\n",
    "for _ in range(num_rounds):\n",
    "  for item in data:\n",
    "    for question1, answer1 in item.items():\n",
    "      for question2, answer2 in item.items():\n",
    "        if question1 != question2:\n",
    "          # Generate embeddings\n",
    "          embedding1 = model.encode([answer1])\n",
    "          embedding2 = model.encode([answer2])\n",
    "\n",
    "          # Calculate cosine similarity\n",
    "          score1 = cosine_similarity(embedding1, model.encode([question1]))[0][0]\n",
    "          score2 = cosine_similarity(embedding2, model.encode([question2]))[0][0]\n",
    "\n",
    "          # Update ELO ratings\n",
    "          if score1 > score2:\n",
    "            elo_ratings[question1], elo_ratings[question2] = update_elo_ratings(elo_ratings[question1], elo_ratings[question2], 1)\n",
    "          elif score1 < score2:\n",
    "            elo_ratings[question1], elo_ratings[question2] = update_elo_ratings(elo_ratings[question1], elo_ratings[question2], 0)\n",
    "          else:\n",
    "            elo_ratings[question1], elo_ratings[question2] = update_elo_ratings(elo_ratings[question1], elo_ratings[question2], 0.5)\n",
    "\n",
    "print(\"ELO ratings:\", elo_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
