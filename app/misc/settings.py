from typing import Final
from langchain.chains.query_constructor.base import AttributeInfo

class Settings:
    TEMPERATURE_GET_PROMPT_WINNER: Final = 0

    NUMBER_OF_PROMPT_CANDIDATES: Final = 3

    HEADERS_TO_SPLIT: Final = [
        ("#", "Topic Header"),
        ("##", "Subtopic Header"),
        ("###", "Paragraph Header")
    ]

    METADATA_INFO: Final = [
        AttributeInfo(
            name="Title",
            description="Part of the document where the text was taken from",
            type="string or list[string]",
        ),
    ]

    CONTENT_DESCRIPTION: Final = "Description of banking products"

    PROMT_TEMPLATE: Final = """
    You are an assistant who answers user questions.
    Use fragments of the received context to answer the question.
    If you don't know the answer, say that you don't know, don't make up an answer.
    Use a maximum of three sentences and be concise.\n
    Question: {question} \n
    Context: {context} \n
    Answer:
    """

    BM25_K: Final = 2
    MMR_K: Final = 2
    MMR_FETCH_K: Final = 5

    RANKING_PROMPT = """
        Your job is to rank the quality of two outputs generated by different prompts. The prompts are used to generate a response for a given task.

        You will be provided with the task description, the test prompt, and two generations - one for each system prompt.

        Rank the generations in order of quality. If Generation A is better, respond with 'A'. If Generation B is better, respond with 'B'.

        Remember, to be considered 'better', a generation must not just be good, it must be noticeably superior to the other.

        Also, keep in mind that you are a very harsh critic. Only rank a generation as better if it truly impresses you more than the other.

        Respond with your ranking, and nothing else. Be fair and unbiased in your judgement.
    """

